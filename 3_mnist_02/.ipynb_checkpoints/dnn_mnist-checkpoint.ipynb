{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goal: accuracy > 98.1% on independent test set, reduction of loss function fluctation using regularization\n",
    "#### Change activation function (ELU)\n",
    "#### Momentum optimizer\n",
    "#### Add He inicialization\n",
    "#### Add Clip values\n",
    "#### Add Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goal: accuracy > 98.1% on independent test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow v > 2 but using compat.v1 (transition code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import os\n",
    "from scipy.stats import reciprocal\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pobranie danych przez API-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(X):\n",
    "    return X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_re, X_test_re = reshape_data(X_train), reshape_data(X_test)\n",
    "X_train_re, X_test_re = X_train_re.astype(float), X_test_re.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wizualizacja przykladowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilosc przykladow uczacych: 60000\n",
      "Ilosc przykladow testowych: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Ilosc przykladow uczacych:', len(X_train))\n",
    "print('Ilosc przykladow testowych:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y, n = None):\n",
    "    if not n:\n",
    "        n = random.randint(0, len(X))\n",
    "    digit, label = X[n], y[n]\n",
    "    plt.imshow(digit, cmap = matplotlib.cm.binary, interpolation = 'nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Label:\" + str(label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset grafu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    seed = random.randint(1, 500)\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNMiniBatchEarlyStoppingTF:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_epochs, layers = [300, 100, 10], batch_size = 100, dropout_rate = 0.025, threshold = 1, learning_rate_init = 0.004, learning_rate_range = (0.003,0.008), n_iterations = 5):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.layers = layers\n",
    "        self.threshold = threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.learning_rate_range = learning_rate_range\n",
    "        self.n_iterations = n_iterations\n",
    "        self.checkpoint_path = \"./tmp/model.ckpt\"\n",
    "        self.checkpoint_epoch_path = self.checkpoint_path + \".epoch\"\n",
    "        self.checkpoint_lr_path = self.checkpoint_path + \".lr\"\n",
    "        self.final_model_path = \"./final_model/model_final.model\"\n",
    "        if not os.path.isdir(\"./tmp\"):\n",
    "            os.mkdir(\"./tmp\")\n",
    "            \n",
    "    def random_batch(self, X_train, y_train, batch_size):\n",
    "        rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "        X_batch = X_train[rnd_indices]\n",
    "        y_batch = y_train[rnd_indices]\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def log_dir(self, prefix=\"\"):\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"./log\"\n",
    "        if prefix:\n",
    "            prefix += \"-\"\n",
    "        name = prefix + \"-\" + now\n",
    "        return \"{}/{}/\".format(root_logdir, name)\n",
    "    \n",
    "    def net(self, X, y, seed = 77, learning_rate = 0.01):\n",
    "        \"\"\"\n",
    "        Model - stworzenie grafu obliczen.\n",
    "        \"\"\"\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        he_init = tf.keras.initializers.VarianceScaling()\n",
    "        training = tf.placeholder_with_default(False, shape=(), name='train_f')\n",
    "        with tf.name_scope(\"DNN_Model\"):\n",
    "            with tf.name_scope(\"net\"):\n",
    "                for layer, i in zip(self.layers[:-1], range(len(self.layers[:-1]))):\n",
    "                    X = tf.layers.dense(X, layer, name = \"hidden\" + str(i), kernel_initializer = he_init)\n",
    "                    X = tf.layers.dropout(X, self.dropout_rate, training = training)\n",
    "                    X = tf.layers.batch_normalization(X, training=training, momentum=0.9)\n",
    "                    X = tf.nn.elu(X)\n",
    "                logits = tf.layers.dense(X, self.layers[-1], kernel_initializer = he_init, name = 'logits') # Bez softmax bo jest zaimplementowana juz w loss function  \n",
    "                logits = tf.layers.batch_normalization(logits, training=training, momentum=0.9)\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "                loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "                loss_summary = tf.summary.scalar('loss', loss)\n",
    "            with tf.name_scope(\"learning\"):\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum = 0.9, use_nesterov = True)\n",
    "                grads_and_vars = optimizer.compute_gradients(loss)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -self.threshold, self.threshold), var)\n",
    "                              for grad, var in grads_and_vars]\n",
    "                training_op = optimizer.apply_gradients(capped_gvs)\n",
    "            with tf.name_scope(\"eval\"):\n",
    "                correct = tf.nn.in_top_k(logits, y, 1)\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "                accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "            with tf.name_scope(\"init\"):\n",
    "                init = tf.global_variables_initializer()\n",
    "            with tf.name_scope(\"save\"):\n",
    "                saver = tf.train.Saver(max_to_keep = 1000000)                        \n",
    "        return training_op, loss, loss_summary, accuracy, accuracy_summary, init, saver\n",
    "                               \n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "        X_* - numpy ndarray\n",
    "        y_* - numpy ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        # wczytanie danych z checkpointu dla optymalizacji learning_rate\n",
    "        # lub inicjalizacja danych poczatkowych\n",
    "        if os.path.isfile(self.checkpoint_epoch_path):\n",
    "            with open(self.checkpoint_lr_path, 'r') as f:\n",
    "                dt = f.read().split(',')\n",
    "                start_iterations = int(dt[0])\n",
    "                lr_best = float(dt[1])\n",
    "                acc_best = float(dt[2])\n",
    "                print('Uczenie wznowione od przebiegu:', start_iterations + 1, '\\tNajlepszy lr', lr_best, '\\tNajlepsze accuracy', acc_best)\n",
    "        else:\n",
    "            start_iterations = 0\n",
    "            lr_best = 0\n",
    "            acc_best = 0\n",
    "            \n",
    "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
    "        n_inputs = int(X_train.shape[1])\n",
    "        \n",
    "\n",
    "        for iteration in range(start_iterations, self.n_iterations):\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                \n",
    "                # folder logu - dane do tensorboard\n",
    "                logdir = self.log_dir('log')\n",
    "                file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "                \n",
    "                best_loss = np.infty\n",
    "                epochs_without_progress = 0\n",
    "                max_epochs_without_progress = 100\n",
    "                \n",
    "                if iteration == 0:\n",
    "                    lr = self.learning_rate_init\n",
    "                else:\n",
    "                    lr = reciprocal(self.learning_rate_range[0], self.learning_rate_range[1]).rvs(random_state = iteration)\n",
    "\n",
    "                print('Przebieg:', iteration + 1)\n",
    "                print('    Logdir:', logdir)\n",
    "                print('    Learning rate:', lr)\n",
    "\n",
    "                with open(self.checkpoint_lr_path, 'w') as f:\n",
    "                    f.write(\"%d, %s, %s\" % (iteration, lr_best, acc_best))\n",
    "\n",
    "                X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "                y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "                training_op, loss, loss_summary, accuracy, accuracy_summary, init, saver = self.net(X, y, learning_rate = lr)                \n",
    "\n",
    "                if os.path.isfile(self.checkpoint_epoch_path):\n",
    "                    with open(self.checkpoint_epoch_path, 'rb') as f:\n",
    "                        start_epoch = int(f.read())\n",
    "                    print('Uczenie wznowione od epoki:', start_epoch)\n",
    "                    saver.restore(sess, self.checkpoint_path)\n",
    "                else:\n",
    "                    start_epoch = 0\n",
    "                    sess.run(init)\n",
    "                \n",
    "                for epoch in range(start_epoch, self.n_epochs):\n",
    "                    for batch_index in range(n_batches):\n",
    "                        X_batch, y_batch = self.random_batch(X_train, y_train, self.batch_size)\n",
    "                        sess.run(training_op, feed_dict = {X : X_batch, y : y_batch})\n",
    "                               \n",
    "                    accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_test, y: y_test})\n",
    "                    file_writer.add_summary(loss_summary_str, epoch)\n",
    "                    file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "                    acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "                               \n",
    "                    if epoch % 10 == 0 and epoch != 0:\n",
    "                        print('        Epoka:', epoch, '\\tLoss:', loss_val, '\\tAccuracy:', acc_test)\n",
    "                        saver.save(sess, self.checkpoint_path)\n",
    "                        with open(self.checkpoint_epoch_path, 'wb') as f:\n",
    "                             f.write(b\"%d\" % (epoch + 1))\n",
    "                               \n",
    "                    # Early stopping \n",
    "                    if epoch % 50 == 0:\n",
    "                        if loss_val < best_loss:\n",
    "                            best_loss = loss_val\n",
    "                            epochs_without_progress = 0\n",
    "                        else:\n",
    "                            epochs_without_progress += 50\n",
    "                            if epochs_without_progress > max_epochs_without_progress:\n",
    "                                print('        Wczesne zatrzymywanie... ')\n",
    "                                print('        Epoka:', epoch, '\\tLoss:', loss_val, '\\tAccuracy:', acc_test)\n",
    "                                saver.save(sess, self.checkpoint_path)\n",
    "                                with open(self.checkpoint_epoch_path, 'wb') as f:\n",
    "                                     f.write(b\"%d\" % (epoch + 1))\n",
    "                                break\n",
    "                            \n",
    "\n",
    "                accuracy_val = sess.run([accuracy], feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "                if accuracy_val[0] > acc_best:\n",
    "                    # Problem z zapisywaniem modelu\n",
    "                    saver.save(sess, self.final_model_path)\n",
    "                    lr_best = lr\n",
    "                    acc_best = accuracy_val[0]\n",
    "\n",
    "                os.remove(self.checkpoint_epoch_path)        \n",
    "                print('    Final iteration accuracy:', accuracy_val[0])\n",
    "        \n",
    "            tf.reset_default_graph()\n",
    "\n",
    "        os.remove(self.checkpoint_lr_path)\n",
    "        self.acc_best = acc_best\n",
    "        self.lr_best = lr_best\n",
    "        print('Final accuracy:', acc_best, 'Learning rate:', lr_best)\n",
    "        \n",
    "    def predict(self, X_score):\n",
    "        \"\"\"\n",
    "        IN: X_score - numpy ndarray\n",
    "        OUT: ndarray \n",
    "        \"\"\"\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(self.final_model_path + '.meta')\n",
    "            saver.restore(sess, self.final_model_path)\n",
    "            graph = tf.get_default_graph()\n",
    "            X = graph.get_tensor_by_name(\"X:0\")\n",
    "            y = graph.get_tensor_by_name(\"y:0\")\n",
    "            logits_ = graph.get_tensor_by_name(\"DNN_Model/net/logits/BiasAdd:0\")\n",
    "            feed_dict = {X : X_score}\n",
    "            logits = sess.run(logits_, feed_dict = feed_dict)\n",
    "            result = tf.nn.softmax(logits, name='softmax').eval()\n",
    "            return [np.argmax(obs) for obs in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNMiniBatchEarlyStoppingTF(n_epochs = 501, layers = [1000, 500, 250, 150,  10], n_iterations = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przebieg: 1\n",
      "    Logdir: ./log/log--20201021130146/\n",
      "    Learning rate: 0.004\n",
      "        Epoka: 10 \tLoss: 0.09796133 \tAccuracy: 0.9773\n",
      "        Epoka: 20 \tLoss: 0.11143364 \tAccuracy: 0.9768\n",
      "        Epoka: 30 \tLoss: 0.10949971 \tAccuracy: 0.9815\n",
      "        Epoka: 40 \tLoss: 0.11681877 \tAccuracy: 0.9838\n",
      "        Epoka: 50 \tLoss: 0.120898195 \tAccuracy: 0.9838\n",
      "        Epoka: 60 \tLoss: 0.12324914 \tAccuracy: 0.9839\n",
      "        Epoka: 70 \tLoss: 0.12557329 \tAccuracy: 0.9841\n",
      "        Epoka: 80 \tLoss: 0.12703443 \tAccuracy: 0.984\n",
      "        Epoka: 90 \tLoss: 0.12834005 \tAccuracy: 0.984\n",
      "        Epoka: 100 \tLoss: 0.12960526 \tAccuracy: 0.984\n",
      "        Epoka: 110 \tLoss: 0.13057698 \tAccuracy: 0.984\n",
      "        Epoka: 120 \tLoss: 0.13147424 \tAccuracy: 0.984\n",
      "        Epoka: 130 \tLoss: 0.13229163 \tAccuracy: 0.9838\n",
      "        Epoka: 140 \tLoss: 0.13302109 \tAccuracy: 0.9838\n",
      "        Epoka: 150 \tLoss: 0.13372387 \tAccuracy: 0.9838\n",
      "        Epoka: 160 \tLoss: 0.1343757 \tAccuracy: 0.9838\n",
      "        Epoka: 170 \tLoss: 0.13497624 \tAccuracy: 0.9836\n",
      "        Epoka: 180 \tLoss: 0.13539451 \tAccuracy: 0.9837\n",
      "        Epoka: 190 \tLoss: 0.13586518 \tAccuracy: 0.9836\n",
      "        Epoka: 200 \tLoss: 0.13637322 \tAccuracy: 0.9837\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.13637322 \tAccuracy: 0.9837\n",
      "    Final iteration accuracy: 0.9837\n",
      "Przebieg: 2\n",
      "    Logdir: ./log/log--20201021134417/\n",
      "    Learning rate: 0.004516058815604455\n",
      "        Epoka: 10 \tLoss: 0.10162945 \tAccuracy: 0.9762\n",
      "        Epoka: 20 \tLoss: 0.10344824 \tAccuracy: 0.9811\n",
      "        Epoka: 30 \tLoss: 0.118011 \tAccuracy: 0.9798\n",
      "        Epoka: 40 \tLoss: 0.11524233 \tAccuracy: 0.9805\n",
      "        Epoka: 50 \tLoss: 0.12256437 \tAccuracy: 0.9814\n",
      "        Epoka: 60 \tLoss: 0.12499719 \tAccuracy: 0.9816\n",
      "        Epoka: 70 \tLoss: 0.12715895 \tAccuracy: 0.9816\n",
      "        Epoka: 80 \tLoss: 0.12876031 \tAccuracy: 0.9817\n",
      "        Epoka: 90 \tLoss: 0.130187 \tAccuracy: 0.9815\n",
      "        Epoka: 100 \tLoss: 0.13138962 \tAccuracy: 0.9816\n",
      "        Epoka: 110 \tLoss: 0.13236006 \tAccuracy: 0.9816\n",
      "        Epoka: 120 \tLoss: 0.13350572 \tAccuracy: 0.9816\n",
      "        Epoka: 130 \tLoss: 0.13435233 \tAccuracy: 0.9816\n",
      "        Epoka: 140 \tLoss: 0.13515644 \tAccuracy: 0.9816\n",
      "        Epoka: 150 \tLoss: 0.13592185 \tAccuracy: 0.9816\n",
      "        Epoka: 160 \tLoss: 0.13657591 \tAccuracy: 0.9816\n",
      "        Epoka: 170 \tLoss: 0.13730726 \tAccuracy: 0.9816\n",
      "        Epoka: 180 \tLoss: 0.13783883 \tAccuracy: 0.9816\n",
      "        Epoka: 190 \tLoss: 0.13844256 \tAccuracy: 0.9816\n",
      "        Epoka: 200 \tLoss: 0.13899331 \tAccuracy: 0.9816\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.13899331 \tAccuracy: 0.9816\n",
      "    Final iteration accuracy: 0.9816\n",
      "Przebieg: 3\n",
      "    Logdir: ./log/log--20201021142646/\n",
      "    Learning rate: 0.004600885766316147\n",
      "        Epoka: 10 \tLoss: 0.102194145 \tAccuracy: 0.9755\n",
      "        Epoka: 20 \tLoss: 0.135188 \tAccuracy: 0.973\n",
      "        Epoka: 30 \tLoss: 0.12371895 \tAccuracy: 0.9794\n",
      "        Epoka: 40 \tLoss: 0.12354959 \tAccuracy: 0.9814\n",
      "        Epoka: 50 \tLoss: 0.12496477 \tAccuracy: 0.9817\n",
      "        Epoka: 60 \tLoss: 0.12889639 \tAccuracy: 0.9818\n",
      "        Epoka: 70 \tLoss: 0.13158305 \tAccuracy: 0.9818\n",
      "        Epoka: 80 \tLoss: 0.13367552 \tAccuracy: 0.9819\n",
      "        Epoka: 90 \tLoss: 0.13552114 \tAccuracy: 0.982\n",
      "        Epoka: 100 \tLoss: 0.1369571 \tAccuracy: 0.982\n",
      "        Epoka: 110 \tLoss: 0.13827999 \tAccuracy: 0.982\n",
      "        Epoka: 120 \tLoss: 0.1394734 \tAccuracy: 0.9821\n",
      "        Epoka: 130 \tLoss: 0.14056827 \tAccuracy: 0.9822\n",
      "        Epoka: 140 \tLoss: 0.14145933 \tAccuracy: 0.9823\n",
      "        Epoka: 150 \tLoss: 0.14227861 \tAccuracy: 0.9824\n",
      "        Epoka: 160 \tLoss: 0.14315239 \tAccuracy: 0.9824\n",
      "        Epoka: 170 \tLoss: 0.14392756 \tAccuracy: 0.9824\n",
      "        Epoka: 180 \tLoss: 0.14462076 \tAccuracy: 0.9826\n",
      "        Epoka: 190 \tLoss: 0.145199 \tAccuracy: 0.9825\n",
      "        Epoka: 200 \tLoss: 0.14582342 \tAccuracy: 0.9823\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.14582342 \tAccuracy: 0.9823\n",
      "    Final iteration accuracy: 0.9823\n",
      "Przebieg: 4\n",
      "    Logdir: ./log/log--20201021150901/\n",
      "    Learning rate: 0.005149249541759541\n",
      "        Epoka: 10 \tLoss: 0.10582653 \tAccuracy: 0.9738\n",
      "        Epoka: 20 \tLoss: 0.1387507 \tAccuracy: 0.9759\n",
      "        Epoka: 30 \tLoss: 0.13259162 \tAccuracy: 0.9772\n",
      "        Epoka: 40 \tLoss: 0.13040319 \tAccuracy: 0.9783\n",
      "        Epoka: 50 \tLoss: 0.1381538 \tAccuracy: 0.9801\n",
      "        Epoka: 60 \tLoss: 0.14306654 \tAccuracy: 0.98\n",
      "        Epoka: 70 \tLoss: 0.14601855 \tAccuracy: 0.9803\n",
      "        Epoka: 80 \tLoss: 0.14851215 \tAccuracy: 0.9804\n",
      "        Epoka: 90 \tLoss: 0.15052922 \tAccuracy: 0.9805\n",
      "        Epoka: 100 \tLoss: 0.15236992 \tAccuracy: 0.9803\n",
      "        Epoka: 110 \tLoss: 0.1539417 \tAccuracy: 0.9803\n",
      "        Epoka: 120 \tLoss: 0.15522596 \tAccuracy: 0.9803\n",
      "        Epoka: 130 \tLoss: 0.15652004 \tAccuracy: 0.9803\n",
      "        Epoka: 140 \tLoss: 0.15772907 \tAccuracy: 0.9805\n",
      "        Epoka: 150 \tLoss: 0.15876816 \tAccuracy: 0.9805\n",
      "        Epoka: 160 \tLoss: 0.15968071 \tAccuracy: 0.9805\n",
      "        Epoka: 170 \tLoss: 0.16051055 \tAccuracy: 0.9806\n",
      "        Epoka: 180 \tLoss: 0.16143094 \tAccuracy: 0.9806\n",
      "        Epoka: 190 \tLoss: 0.16221039 \tAccuracy: 0.9805\n",
      "        Epoka: 200 \tLoss: 0.16294548 \tAccuracy: 0.9805\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.16294548 \tAccuracy: 0.9805\n",
      "    Final iteration accuracy: 0.9805\n",
      "Przebieg: 5\n",
      "    Logdir: ./log/log--20201021155230/\n",
      "    Learning rate: 0.007745433495437058\n",
      "        Epoka: 10 \tLoss: 0.1367649 \tAccuracy: 0.9641\n",
      "        Epoka: 20 \tLoss: 0.14118236 \tAccuracy: 0.9676\n",
      "        Epoka: 30 \tLoss: 0.16049118 \tAccuracy: 0.9695\n",
      "        Epoka: 40 \tLoss: 0.16855116 \tAccuracy: 0.9704\n",
      "        Epoka: 50 \tLoss: 0.16218746 \tAccuracy: 0.9727\n",
      "        Epoka: 60 \tLoss: 0.19095981 \tAccuracy: 0.973\n",
      "        Epoka: 70 \tLoss: 0.20788784 \tAccuracy: 0.9742\n",
      "        Epoka: 80 \tLoss: 0.19463797 \tAccuracy: 0.9708\n",
      "        Epoka: 90 \tLoss: 0.21840715 \tAccuracy: 0.9721\n",
      "        Epoka: 100 \tLoss: 0.22356844 \tAccuracy: 0.9704\n",
      "        Epoka: 110 \tLoss: 0.2022501 \tAccuracy: 0.9753\n",
      "        Epoka: 120 \tLoss: 0.21610557 \tAccuracy: 0.9754\n",
      "        Epoka: 130 \tLoss: 0.22397354 \tAccuracy: 0.9751\n",
      "        Epoka: 140 \tLoss: 0.23027432 \tAccuracy: 0.9751\n",
      "        Epoka: 150 \tLoss: 0.23490512 \tAccuracy: 0.9752\n",
      "        Epoka: 160 \tLoss: 0.23932014 \tAccuracy: 0.9752\n",
      "        Epoka: 170 \tLoss: 0.24283054 \tAccuracy: 0.975\n",
      "        Epoka: 180 \tLoss: 0.24587229 \tAccuracy: 0.975\n",
      "        Epoka: 190 \tLoss: 0.24860196 \tAccuracy: 0.975\n",
      "        Epoka: 200 \tLoss: 0.25082472 \tAccuracy: 0.975\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.25082472 \tAccuracy: 0.975\n",
      "    Final iteration accuracy: 0.975\n",
      "Final accuracy: 0.9837 Learning rate: 0.004\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model.train(X_train = X_train_re, y_train = y_train, X_test = X_test_re, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./final_model/model_final.model\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y, pred, n = None):\n",
    "    if not n:\n",
    "        n = random.randint(0, len(X))\n",
    "    digit, label, pred = X[n], y[n], pred[n]\n",
    "    plt.imshow(digit, cmap = matplotlib.cm.binary, interpolation = 'nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Label:\" + str(label) + \" Pred:\" + str(pred))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(X = X_test, y = y_test, pred = pred, n = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projec goal has been archieved (accuracy: ~ 98.4 %)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
