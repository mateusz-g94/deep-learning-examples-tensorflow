{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project goal: accuracy > 98% on independent test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow v > 2 but using compat.v1 (transition code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import os\n",
    "from scipy.stats import reciprocal\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pobranie danych przez API-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(X):\n",
    "    return X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_re, X_test_re = reshape_data(X_train), reshape_data(X_test)\n",
    "X_train_re, X_test_re = X_train_re.astype(float), X_test_re.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wizualizacja przykladowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilosc przykladow uczacych: 60000\n",
      "Ilosc przykladow testowych: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Ilosc przykladow uczacych:', len(X_train))\n",
    "print('Ilosc przykladow testowych:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y, n = None):\n",
    "    if not n:\n",
    "        n = random.randint(0, len(X))\n",
    "    digit, label = X[n], y[n]\n",
    "    plt.imshow(digit, cmap = matplotlib.cm.binary, interpolation = 'nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Label:\" + str(label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset grafu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def reset_graph():\n",
    "    tf.reset_default_graph()\n",
    "    seed = random.randint(1, 500)\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standaryzacja danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNMiniBatchEarlyStoppingTF:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_epochs, layers = [300, 100, 10], batch_size = 100, learning_rate_init = 0.0007, learning_rate_range = (0.0005, 0.001), n_iterations = 5):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.learning_rate_range = learning_rate_range\n",
    "        self.n_iterations = n_iterations\n",
    "        self.checkpoint_path = \"./tmp/model.ckpt\"\n",
    "        self.checkpoint_epoch_path = self.checkpoint_path + \".epoch\"\n",
    "        self.checkpoint_lr_path = self.checkpoint_path + \".lr\"\n",
    "        self.final_model_path = \"./final_model/model_final.model\"\n",
    "        if not os.path.isdir(\"./tmp\"):\n",
    "            os.mkdir(\"./tmp\")\n",
    "            \n",
    "    def random_batch(self, X_train, y_train, batch_size):\n",
    "        rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "        X_batch = X_train[rnd_indices]\n",
    "        y_batch = y_train[rnd_indices]\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def log_dir(self, prefix=\"\"):\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"./log\"\n",
    "        if prefix:\n",
    "            prefix += \"-\"\n",
    "        name = prefix + \"-\" + now\n",
    "        return \"{}/{}/\".format(root_logdir, name)\n",
    "    \n",
    "    def net(self, X, y, seed = 77, learning_rate = 0.01):\n",
    "        \"\"\"\n",
    "        Model - stworzenie grafu obliczen.\n",
    "        \"\"\"\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        with tf.name_scope(\"DNN_Model\"):\n",
    "            with tf.name_scope(\"net\"):\n",
    "                hidden_flg = False\n",
    "                if self.layers:\n",
    "                    for layer in self.layers[:-1]:\n",
    "                        if not hidden_flg:\n",
    "                            hidden = tf.layers.dense(X, layer, name = \"hidden\" + str(layer), activation = tf.nn.relu)\n",
    "                            hidden_flg = True\n",
    "                        else:\n",
    "                            hidden = tf.layers.dense(hidden, layer, name = \"hidden\" + str(layer), activation = tf.nn.relu)\n",
    "                if hidden_flg:\n",
    "                    logits = tf.layers.dense(hidden, self.layers[-1], name = 'logits') # Bez softmax bo jest zaimplementowana juz w loss function\n",
    "                else:\n",
    "                    logits = tf.layers.dense(X, self.layers[-1], name = 'logits') # Bez softmax bo jest zaimplementowana juz w loss function\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "                loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "                loss_summary = tf.summary.scalar('loss', loss)\n",
    "            with tf.name_scope(\"learning\"):\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "                training_op = optimizer.minimize(loss)\n",
    "            with tf.name_scope(\"eval\"):\n",
    "                correct = tf.nn.in_top_k(logits, y, 1)\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "                accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "            with tf.name_scope(\"init\"):\n",
    "                init = tf.global_variables_initializer()\n",
    "            with tf.name_scope(\"save\"):\n",
    "                saver = tf.train.Saver(max_to_keep = 1000000)                        \n",
    "        return training_op, loss, loss_summary, accuracy, accuracy_summary, init, saver\n",
    "                               \n",
    "    def train(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"\n",
    "        X_* - numpy ndarray\n",
    "        y_* - numpy ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        # wczytanie danych z checkpointu dla optymalizacji learning_rate\n",
    "        # lub inicjalizacja danych poczatkowych\n",
    "        if os.path.isfile(self.checkpoint_epoch_path):\n",
    "            with open(self.checkpoint_lr_path, 'r') as f:\n",
    "                dt = f.read().split(',')\n",
    "                start_iterations = int(dt[0])\n",
    "                lr_best = float(dt[1])\n",
    "                acc_best = float(dt[2])\n",
    "                print('Uczenie wznowione od przebiegu:', start_iterations + 1, '\\tNajlepszy lr', lr_best, '\\tNajlepsze accuracy', acc_best)\n",
    "        else:\n",
    "            start_iterations = 0\n",
    "            lr_best = 0\n",
    "            acc_best = 0\n",
    "            \n",
    "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
    "        n_inputs = int(X_train.shape[1])\n",
    "        \n",
    "\n",
    "        for iteration in range(start_iterations, self.n_iterations):\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                \n",
    "                # folder logu - dane do tensorboard\n",
    "                logdir = self.log_dir('log')\n",
    "                file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "                \n",
    "                best_loss = np.infty\n",
    "                epochs_without_progress = 0\n",
    "                max_epochs_without_progress = 100\n",
    "                \n",
    "                if iteration == 0:\n",
    "                    lr = self.learning_rate_init\n",
    "                else:\n",
    "                    lr = reciprocal(self.learning_rate_range[0], self.learning_rate_range[1]).rvs(random_state = iteration)\n",
    "\n",
    "                print('Przebieg:', iteration + 1)\n",
    "                print('    Logdir:', logdir)\n",
    "                print('    Learning rate:', lr)\n",
    "\n",
    "                with open(self.checkpoint_lr_path, 'w') as f:\n",
    "                    f.write(\"%d, %s, %s\" % (iteration, lr_best, acc_best))\n",
    "\n",
    "                X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "                y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "                training_op, loss, loss_summary, accuracy, accuracy_summary, init, saver = self.net(X, y, learning_rate = lr)                \n",
    "\n",
    "                if os.path.isfile(self.checkpoint_epoch_path):\n",
    "                    with open(self.checkpoint_epoch_path, 'rb') as f:\n",
    "                        start_epoch = int(f.read())\n",
    "                    print('Uczenie wznowione od epoki:', start_epoch)\n",
    "                    saver.restore(sess, self.checkpoint_path)\n",
    "                else:\n",
    "                    start_epoch = 0\n",
    "                    sess.run(init)\n",
    "                \n",
    "                for epoch in range(start_epoch, self.n_epochs):\n",
    "                    for batch_index in range(n_batches):\n",
    "                        X_batch, y_batch = self.random_batch(X_train, y_train, self.batch_size)\n",
    "                        sess.run(training_op, feed_dict = {X : X_batch, y : y_batch})\n",
    "                               \n",
    "                    accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_test, y: y_test})\n",
    "                    file_writer.add_summary(loss_summary_str, epoch)\n",
    "                    file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "                    acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "                               \n",
    "                    if epoch % 10 == 0 and epoch != 0:\n",
    "                        print('        Epoka:', epoch, '\\tLoss:', loss_val, '\\tAccuracy:', acc_test)\n",
    "                        saver.save(sess, self.checkpoint_path)\n",
    "                        with open(self.checkpoint_epoch_path, 'wb') as f:\n",
    "                             f.write(b\"%d\" % (epoch + 1))\n",
    "                               \n",
    "                    # Early stopping \n",
    "                    if epoch % 50 == 0:\n",
    "                        if loss_val < best_loss:\n",
    "                            best_loss = loss_val\n",
    "                            epochs_without_progress = 0\n",
    "                        else:\n",
    "                            epochs_without_progress += 50\n",
    "                            if epochs_without_progress > max_epochs_without_progress:\n",
    "                                print('        Wczesne zatrzymywanie... ')\n",
    "                                print('        Epoka:', epoch, '\\tLoss:', loss_val, '\\tAccuracy:', acc_test)\n",
    "                                saver.save(sess, self.checkpoint_path)\n",
    "                                with open(self.checkpoint_epoch_path, 'wb') as f:\n",
    "                                     f.write(b\"%d\" % (epoch + 1))\n",
    "                                break\n",
    "                            \n",
    "\n",
    "                accuracy_val = sess.run([accuracy], feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "                if accuracy_val[0] > acc_best:\n",
    "                    # Problem z zapisywaniem modelu\n",
    "                    saver.save(sess, self.final_model_path)\n",
    "                    lr_best = lr\n",
    "                    acc_best = accuracy_val[0]\n",
    "\n",
    "                os.remove(self.checkpoint_epoch_path)        \n",
    "                print('    Final iteration accuracy:', accuracy_val[0])\n",
    "        \n",
    "            tf.reset_default_graph()\n",
    "\n",
    "        os.remove(self.checkpoint_lr_path)\n",
    "        self.acc_best = acc_best\n",
    "        self.lr_best = lr_best\n",
    "        print('Final accuracy:', acc_best, 'Learning rate:', lr_best)\n",
    "        \n",
    "    def predict(self, X_score):\n",
    "        \"\"\"\n",
    "        IN: X_score - numpy ndarray\n",
    "        OUT: ndarray \n",
    "        \"\"\"\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.import_meta_graph(self.final_model_path + '.meta')\n",
    "            saver.restore(sess, self.final_model_path)\n",
    "            graph = tf.get_default_graph()\n",
    "            X = graph.get_tensor_by_name(\"X:0\")\n",
    "            y = graph.get_tensor_by_name(\"y:0\")\n",
    "            logits_ = graph.get_tensor_by_name(\"DNN_Model/net/logits/BiasAdd:0\")\n",
    "            feed_dict = {X : X_score}\n",
    "            logits = sess.run(logits_, feed_dict = feed_dict)\n",
    "            result = tf.nn.softmax(logits, name='softmax').eval()\n",
    "            return [np.argmax(obs) for obs in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNMiniBatchEarlyStoppingTF(n_epochs = 501, layers = [400, 250, 75,  10], n_iterations = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przebieg: 1\n",
      "    Logdir: ./log/log--20201011132911/\n",
      "    Learning rate: 0.0007\n",
      "        Epoka: 10 \tLoss: 0.1457413 \tAccuracy: 0.9692\n",
      "        Epoka: 20 \tLoss: 0.13544977 \tAccuracy: 0.9762\n",
      "        Epoka: 30 \tLoss: 0.14992721 \tAccuracy: 0.9752\n",
      "        Epoka: 40 \tLoss: 0.16577837 \tAccuracy: 0.978\n",
      "        Epoka: 50 \tLoss: 0.13852338 \tAccuracy: 0.9805\n",
      "        Epoka: 60 \tLoss: 0.20206475 \tAccuracy: 0.9802\n",
      "        Epoka: 70 \tLoss: 0.19693762 \tAccuracy: 0.9797\n",
      "        Epoka: 80 \tLoss: 0.20983095 \tAccuracy: 0.981\n",
      "        Epoka: 90 \tLoss: 0.24674188 \tAccuracy: 0.9814\n",
      "        Epoka: 100 \tLoss: 0.3372114 \tAccuracy: 0.9807\n",
      "        Epoka: 110 \tLoss: 0.27258673 \tAccuracy: 0.9821\n",
      "        Epoka: 120 \tLoss: 0.36819288 \tAccuracy: 0.9793\n",
      "        Epoka: 130 \tLoss: 0.3451548 \tAccuracy: 0.9806\n",
      "        Epoka: 140 \tLoss: 0.37252402 \tAccuracy: 0.979\n",
      "        Epoka: 150 \tLoss: 0.4128062 \tAccuracy: 0.9806\n",
      "        Epoka: 160 \tLoss: 0.43438506 \tAccuracy: 0.9819\n",
      "        Epoka: 170 \tLoss: 0.4525698 \tAccuracy: 0.9823\n",
      "        Epoka: 180 \tLoss: 0.36831725 \tAccuracy: 0.9816\n",
      "        Epoka: 190 \tLoss: 0.45668644 \tAccuracy: 0.9832\n",
      "        Epoka: 200 \tLoss: 0.4489197 \tAccuracy: 0.9809\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.4489197 \tAccuracy: 0.9809\n",
      "    Final iteration accuracy: 0.9809\n",
      "Przebieg: 2\n",
      "    Logdir: ./log/log--20201011134412/\n",
      "    Learning rate: 0.0006675843338987175\n",
      "        Epoka: 10 \tLoss: 0.16244489 \tAccuracy: 0.9677\n",
      "        Epoka: 20 \tLoss: 0.12163352 \tAccuracy: 0.9782\n",
      "        Epoka: 30 \tLoss: 0.1355429 \tAccuracy: 0.9774\n",
      "        Epoka: 40 \tLoss: 0.14149472 \tAccuracy: 0.9794\n",
      "        Epoka: 50 \tLoss: 0.18772432 \tAccuracy: 0.976\n",
      "        Epoka: 60 \tLoss: 0.2086885 \tAccuracy: 0.9754\n",
      "        Epoka: 70 \tLoss: 0.20055112 \tAccuracy: 0.9806\n",
      "        Epoka: 80 \tLoss: 0.23398164 \tAccuracy: 0.9796\n",
      "        Epoka: 90 \tLoss: 0.20921539 \tAccuracy: 0.9796\n",
      "        Epoka: 100 \tLoss: 0.22286074 \tAccuracy: 0.9817\n",
      "        Epoka: 110 \tLoss: 0.18940613 \tAccuracy: 0.9814\n",
      "        Epoka: 120 \tLoss: 0.28896034 \tAccuracy: 0.9806\n",
      "        Epoka: 130 \tLoss: 0.284505 \tAccuracy: 0.9765\n",
      "        Epoka: 140 \tLoss: 0.24581707 \tAccuracy: 0.9789\n",
      "        Epoka: 150 \tLoss: 0.35288742 \tAccuracy: 0.9791\n",
      "        Epoka: 160 \tLoss: 0.35518616 \tAccuracy: 0.9795\n",
      "        Epoka: 170 \tLoss: 0.30760953 \tAccuracy: 0.9822\n",
      "        Epoka: 180 \tLoss: 0.3191633 \tAccuracy: 0.9822\n",
      "        Epoka: 190 \tLoss: 0.36021733 \tAccuracy: 0.981\n",
      "        Epoka: 200 \tLoss: 0.39844576 \tAccuracy: 0.9806\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.39844576 \tAccuracy: 0.9806\n",
      "    Final iteration accuracy: 0.9806\n",
      "Przebieg: 3\n",
      "    Logdir: ./log/log--20201011135938/\n",
      "    Learning rate: 0.0006764217253401736\n",
      "        Epoka: 10 \tLoss: 0.14448401 \tAccuracy: 0.9714\n",
      "        Epoka: 20 \tLoss: 0.15052414 \tAccuracy: 0.974\n",
      "        Epoka: 30 \tLoss: 0.14124048 \tAccuracy: 0.9785\n",
      "        Epoka: 40 \tLoss: 0.12589248 \tAccuracy: 0.9812\n",
      "        Epoka: 50 \tLoss: 0.15602304 \tAccuracy: 0.9804\n",
      "        Epoka: 60 \tLoss: 0.19478147 \tAccuracy: 0.9802\n",
      "        Epoka: 70 \tLoss: 0.18189397 \tAccuracy: 0.9794\n",
      "        Epoka: 80 \tLoss: 0.24422927 \tAccuracy: 0.9779\n",
      "        Epoka: 90 \tLoss: 0.23932339 \tAccuracy: 0.9802\n",
      "        Epoka: 100 \tLoss: 0.2998329 \tAccuracy: 0.9778\n",
      "        Epoka: 110 \tLoss: 0.24146616 \tAccuracy: 0.9792\n",
      "        Epoka: 120 \tLoss: 0.23987986 \tAccuracy: 0.9793\n",
      "        Epoka: 130 \tLoss: 0.23514424 \tAccuracy: 0.9807\n",
      "        Epoka: 140 \tLoss: 0.28833595 \tAccuracy: 0.9802\n",
      "        Epoka: 150 \tLoss: 0.30416074 \tAccuracy: 0.9806\n",
      "        Epoka: 160 \tLoss: 0.37071377 \tAccuracy: 0.9816\n",
      "        Epoka: 170 \tLoss: 0.3961346 \tAccuracy: 0.9794\n",
      "        Epoka: 180 \tLoss: 0.25803167 \tAccuracy: 0.9798\n",
      "        Epoka: 190 \tLoss: 0.41372955 \tAccuracy: 0.9798\n",
      "        Epoka: 200 \tLoss: 0.42833418 \tAccuracy: 0.9798\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.42833418 \tAccuracy: 0.9798\n",
      "    Final iteration accuracy: 0.9798\n",
      "Przebieg: 4\n",
      "    Logdir: ./log/log--20201011141454/\n",
      "    Learning rate: 0.0007324478264390343\n",
      "        Epoka: 10 \tLoss: 0.13308033 \tAccuracy: 0.9678\n",
      "        Epoka: 20 \tLoss: 0.14077774 \tAccuracy: 0.9754\n",
      "        Epoka: 30 \tLoss: 0.11839358 \tAccuracy: 0.978\n",
      "        Epoka: 40 \tLoss: 0.17274424 \tAccuracy: 0.9775\n",
      "        Epoka: 50 \tLoss: 0.15087382 \tAccuracy: 0.9809\n",
      "        Epoka: 60 \tLoss: 0.20982732 \tAccuracy: 0.9786\n",
      "        Epoka: 70 \tLoss: 0.17281419 \tAccuracy: 0.9802\n",
      "        Epoka: 80 \tLoss: 0.20219229 \tAccuracy: 0.9788\n",
      "        Epoka: 90 \tLoss: 0.2173478 \tAccuracy: 0.9746\n",
      "        Epoka: 100 \tLoss: 0.20046408 \tAccuracy: 0.9817\n",
      "        Epoka: 110 \tLoss: 0.2886634 \tAccuracy: 0.981\n",
      "        Epoka: 120 \tLoss: 0.29808038 \tAccuracy: 0.9818\n",
      "        Epoka: 130 \tLoss: 0.3250767 \tAccuracy: 0.9819\n",
      "        Epoka: 140 \tLoss: 0.29251212 \tAccuracy: 0.9772\n",
      "        Epoka: 150 \tLoss: 0.3687649 \tAccuracy: 0.98\n",
      "        Epoka: 160 \tLoss: 0.36649913 \tAccuracy: 0.9825\n",
      "        Epoka: 170 \tLoss: 0.31218785 \tAccuracy: 0.9801\n",
      "        Epoka: 180 \tLoss: 0.3898955 \tAccuracy: 0.9791\n",
      "        Epoka: 190 \tLoss: 0.49037397 \tAccuracy: 0.9812\n",
      "        Epoka: 200 \tLoss: 0.37306547 \tAccuracy: 0.9789\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.37306547 \tAccuracy: 0.9789\n",
      "    Final iteration accuracy: 0.9789\n",
      "Przebieg: 5\n",
      "    Logdir: ./log/log--20201011142944/\n",
      "    Learning rate: 0.000977405981721752\n",
      "        Epoka: 10 \tLoss: 0.12509966 \tAccuracy: 0.9711\n",
      "        Epoka: 20 \tLoss: 0.12763618 \tAccuracy: 0.9728\n",
      "        Epoka: 30 \tLoss: 0.14834698 \tAccuracy: 0.9794\n",
      "        Epoka: 40 \tLoss: 0.1676167 \tAccuracy: 0.9779\n",
      "        Epoka: 50 \tLoss: 0.18752739 \tAccuracy: 0.9801\n",
      "        Epoka: 60 \tLoss: 0.18161832 \tAccuracy: 0.9806\n",
      "        Epoka: 70 \tLoss: 0.22442183 \tAccuracy: 0.9746\n",
      "        Epoka: 80 \tLoss: 0.29657295 \tAccuracy: 0.9794\n",
      "        Epoka: 90 \tLoss: 0.2837471 \tAccuracy: 0.9796\n",
      "        Epoka: 100 \tLoss: 0.367619 \tAccuracy: 0.9763\n",
      "        Epoka: 110 \tLoss: 0.30291942 \tAccuracy: 0.9811\n",
      "        Epoka: 120 \tLoss: 0.34944633 \tAccuracy: 0.9805\n",
      "        Epoka: 130 \tLoss: 0.3547416 \tAccuracy: 0.9807\n",
      "        Epoka: 140 \tLoss: 0.5317273 \tAccuracy: 0.9771\n",
      "        Epoka: 150 \tLoss: 0.58840483 \tAccuracy: 0.9806\n",
      "        Epoka: 160 \tLoss: 0.3382684 \tAccuracy: 0.9789\n",
      "        Epoka: 170 \tLoss: 0.4695209 \tAccuracy: 0.9754\n",
      "        Epoka: 180 \tLoss: 0.46055743 \tAccuracy: 0.9702\n",
      "        Epoka: 190 \tLoss: 0.686604 \tAccuracy: 0.98\n",
      "        Epoka: 200 \tLoss: 0.6547683 \tAccuracy: 0.9725\n",
      "        Wczesne zatrzymywanie... \n",
      "        Epoka: 200 \tLoss: 0.6547683 \tAccuracy: 0.9725\n",
      "    Final iteration accuracy: 0.9725\n",
      "Final accuracy: 0.9809 Learning rate: 0.0007\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model.train(X_train = X_train_re, y_train = y_train, X_test = X_test_re, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./final_model/model_final.model\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(X, y, pred, n = None):\n",
    "    if not n:\n",
    "        n = random.randint(0, len(X))\n",
    "    digit, label, pred = X[n], y[n], pred[n]\n",
    "    plt.imshow(digit, cmap = matplotlib.cm.binary, interpolation = 'nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Label:\" + str(label) + \" Pred:\" + str(pred))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(X = X_test, y = y_test, pred = pred, n = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projec goal has been archieved (accuracy: ~ 98.1 %)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
